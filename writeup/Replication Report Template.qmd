---
title: "Replication of Study 3 by Walker et al. (2019, Judgment and Decision Making)"
author: "Aya Salim (ayasalim@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

## Introduction

### Justification

My research interests lie in how people process and trust information in complex media environments. One phenomenon I am especially interested in is why some individuals find misleading claims compelling. Walker et al. (2019) propose that illusory pattern perception - the tendency to perceive meaning or structure in randomness - may help explain people’s openness to pseudo-profound statements. Replicating Study 3 from this paper aligns closely with my work on misinformation and trust by testing a cognitive mechanism that could underlie belief in misinformation or misplaced confidence in untrustworthy sources.

I selected Study 3 particularly because it offers the most integrative test of the authors' argument: it combines multiple measures of pattern perception (both perceptual and inferential) with assessments of analytic thinking and pseudo-profound belief. The earlier experiments establish foundational correlations, but Experiment 3 provides the strongest evidence for generalizability across tasks and cognitive domains. Replicating this broader design allows for a more comprehensive test of whether the relationship between pattern perception and receptivity to pseudo-profound statements is robust and reproducible. I also hope to extend the experiment by adding trust or misinformation measures to examine whether the same cognitive bias that drives "bullshit receptivity" also predicts susceptibility to misinformation.

Given that research connecting illusory pattern perception to conspiracy beliefs and credulity has already produced mixed results, a careful replication of this paradigm can provide valuable clarity about the reliability and boundary conditions of the results. In doing so, this contributes not only to my own program of research but also to broader efforts to assess the reproducibility of psychological findings about belief formation and illusory pattern perception.

### Stimuli and Procedures

The experiment linked simple perception and reasoning tasks to judgments of meaning. Participants viewed ambiguous or random stimuli and made basic perceptual or causal judgments, then rated how “profound” various statements seemed and completed a brief reasoning test. The key result I hope to replicate was that people who more often perceive patterns in randomness also rate pseudo-profound statements as more meaningful.

In the original study, participants completed two tasks designed to measure illusory pattern perception, followed by two tasks assessing reasoning and belief tendencies.

**Snowy Pictures Task (SPT)**. Participants viewed a series of 24 blurry, black-and-white “snowy” images - half contained a faintly embedded object, and half were pure visual noise. On each trial, participants indicated whether they thought an object was present. The key measure was the number of "false-alarm" responses - seeing an object in images that contain only random noise.

**Co-variation Task**. Participants read short vignettes describing potential causal relationships between a food additive and a disease. Each vignette was accompanied by a simple 2x2 table showing how often the disease occurred among people who did or did not consume the additive. In some cases, a real statistical relationship was present; in others, the data were random. Participants rated how strongly they believed the additive caused or prevented the disease using a slider ranging from −100 ("strongly prevents") to +100 ("strongly causes"). Over-attributing causality in the random tables reflected illusory pattern perception.

**Profundity Ratings and Analytic Thinking**. Participants rated the "profoundness" of pseudo-profound statements and genuine inspirational quotes, and completed a brief cognitive-reflection test (CRT) that measures analytic reasoning.

**Possible Extension: Headline Accuracy Task**. Participants will see a balanced set of real and false news headlines and rate how accurate each seems. Stimuli will be drawn from published misinformation studies and pre-tested to ensure moderate believability. The key outcome is perceived accuracy of the false headlines.

For the replication, I will recreate the same structure using Qualtrics and a Prolific participant pool. Primary challenges will be ensuring that the stimuli work reliably online and that participants stay attentive throughout the task, especially given my goal to extend this experiment with other tasks/measures related to misinformation. Ensuring participants understand the slider scale in the co-variation task may also pose a challenge.

### Links

- **Repository:** [https://github.com/psych251/walker2019](https://github.com/psych251/walker2019)  
- **Original paper:** [https://github.com/psych251/walker2019/blob/main/original_paper/finding-meaning-in-the-clouds-illusory-pattern-perception-predicts-receptivity-to-pseudo-profound-bullshit.pdf](https://github.com/psych251/walker2019/blob/main/original_paper/finding-meaning-in-the-clouds-illusory-pattern-perception-predicts-receptivity-to-pseudo-profound-bullshit.pdf)


## Methods

### Power Analysis

Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size.

### Planned Sample

Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any.

### Materials

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

### Procedure	

Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

### Analysis Plan

Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.

### Differences from Original Study

Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


## Results


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
